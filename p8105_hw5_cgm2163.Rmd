---
title: "Homework 5"
output: github_document
---

**Set up**

```{r setup, include = FALSE}

library(tidyverse)
library(rvest)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = 0.6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.color = "virdis",
  ggplot2.continuous.color = "viridis"
  
)

scale_color_discrete = scale_color_viridis_d
scale_fill_discrete = scale_fill_viridis_d


```

## Problem 1

This data was collected from the Washington Post regarding homicides in 50 large
US cities. The data details a number of cases and their status, whether they
were closed by arrest, closed with no arrest, or open/no arrest.

We can read in the data from the Washington Post to learn more.


```{r}

homicide_data = 
  read_csv("./homicide-data.csv") %>%
  mutate(
    city_state = str_c(city, state, sep = "_"),
    resolved = case_when(
        disposition == "Closed without arrest" ~ "unsolved",
        disposition == "Open/No arrest"        ~ "unsolved",
        disposition == "Closed by arrest"      ~ "solved",
      
    )
  ) %>%
  select(city_state, resolved) %>%
  filter(city_state != "Tulsa_AL")


view(homicide_data)


```

now we can see how many homicides and how many unsolved homicides are in each city

```{r}

summary_data = 
  homicide_data %>%
  group_by(city_state) %>%
  summarize(
    hom_total = n(),
    hom_unsolved = sum(resolved == "unsolved")
  )
  
view(summary_data)

```

Let's look at our Baltimore data!

```{r}

baltimore_prop_test =
  prop.test(
  summary_data %>%
    filter(city_state == "Baltimore_MD") %>%
    pull(hom_unsolved),
    summary_data %>%
      filter(city_state == "Baltimore_MD") %>%
        pull(hom_total))


baltimore_prop_test %>%
  broom::tidy() %>%
  knitr::kable()


```

For Baltimore, Maryland, the estimated proportion of homicides that are unsolved
is 0.646. The confidence interval around this is (0.628, 0.663), meaning that
we are 95% sure that the true proportion of homicides that are unsolved is
between 0.628 and 0.663.


We can now do the same analysis for all of the cities in our data frame:

```{r}

prop_test_cities = 
  function(homicide_data){
    cities = 
      homicide_data %>%
      summarize(
        unsolved = sum(resolved == "unsolved"),
        count = n()
      )
    
    cities_test = 
      prop.test(
        x = cities %>%
          pull(unsolved),
        n = cities %>%
          pull(count)
      )
    
    
    return(cities_test)
    
  }

cities_test_results = 
  homicide_data %>%
    nest(data = resolved) %>%
    mutate(
      test_results = map(data, prop_test_cities),
      tidy_output = map(test_results, broom::tidy)
    ) %>%
  select(city_state, tidy_output) %>%
  unnest(tidy_output) %>%
  select(city_state, estimate, starts_with("conf"))

head(cities_test_results) %>%
  knitr::kable()

```

graph!

```{r}

cities_test_results %>%
  mutate(city_state = fct_reorder(city_state, estimate)) %>%
  ggplot(aes(x = city_state, y = estimate)) + geom_point() + geom_errorbar(
    aes(ymin = conf.low, ymax = conf.high)) + theme(axis.text.x = element_text(
      angle = 90, vjust = 0.5, hjust = 1))

```


## Problem 2

We have been presented with a zip file full of longitudinal study data. We can
read in and tidy this data so that it is more easily understood and better for
graphical visualization.

```{r}

longitudinal_study =
  tibble(file_name = list.files(path = "./data", full.names = TRUE)) %>%
  mutate(data = purrr::map(.x = file_name, ~read_csv(.x))) %>%
  unnest(data) %>%
    separate(file_name, c("file_1", "file_2", "name"), sep = "/") %>%
    separate(name, c("arm", "id", "csv")) %>%
      mutate(wing = recode(arm, con = "control", exp = "experimental")) %>%
  select(-file_1, -file_2, -csv, -arm) %>%
  pivot_longer(week_1:week_8,
               names_to = "week",
               values_to = "data_point",
               names_prefix = "week_"
               ) %>%
  mutate(as.factor(week))

```

Now we can use the resulting dataframe, _longitudinal_study_, to create a 
spaghetti plot so we can better understand the trends.

```{r}

longitudinal_study %>%
  ggplot(aes(x = week, y = data_point, group = id, color = id)) + geom_line() +
  theme(legend.position = "right") +
  labs(
    x = "Week of Observation",
    y = "Data Value Collected by Wing",
    title = "Participant Data by Week"
  ) + facet_grid(cols = vars(wing))


```

From this graph, it is easy to see that the values in the experimental group are
much higher than the control group, and the upward trend in points among the 
experimental group are much more consistent. It almost appears that the majority
of the control group trend downwards as the weeks go on, whereas the 
experimental group almost all grow up.



##Question 3







